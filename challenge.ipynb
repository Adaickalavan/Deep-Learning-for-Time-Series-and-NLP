{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for Time Series and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Lambda, Bidirectional, Conv1D, MaxPooling1D\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readIn(filepath):\n",
    "    '''\n",
    "    Arguments:\n",
    "    filepath -- path of file to be read\n",
    "\n",
    "    Returns:    \n",
    "    x -- list of strings read from the input file    \n",
    "    '''\n",
    "    # Read in the files\n",
    "    with open(filepath, 'r') as x:\n",
    "        x = x.read()\n",
    "        x = x.splitlines()  # Split files into sentences\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_traindev = readIn('./xtrain.txt')\n",
    "y_traindev = readIn('./ytrain.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this python notebook, we shall use this notation:\n",
    "* m is number of examples\n",
    "* n_v is vocabulary size (i.e., vocab_size)\n",
    "* n_c is number of classes (i.e., number of novels to predict)\n",
    "* n_a is dimensionality of the hidden state in LSTM cell\n",
    "* Tx is length of each string. All strings will be appended or truncated to the same length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the desired string length and number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n"
     ]
    }
   ],
   "source": [
    "# Compute the maximum string length\n",
    "Txmax = len(max(x_traindev, key=len))\n",
    "print(Txmax)\n",
    "\n",
    "# We set desired string length to 448 which is 5 char shorter than Txmax. \n",
    "# This simplifies the dimensions during convolution later, without significant performance loss\n",
    "Tx = 448\n",
    "# Set the number of classes\n",
    "n_c = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence in the data set is a sequence of characters. Hence, we will build a character level sentence classification model.\n",
    "\n",
    "First, build a character dictionary. Then, convert each training sample (i.e., sentence) from character sequence to integer sequence using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def char2ix(x):\n",
    "    '''\n",
    "    Arguments:\n",
    "    x -- list of strings\n",
    "    \n",
    "    Returns:\n",
    "    char_to_ix -- dictionary mapping character to integer\n",
    "    n_v -- vocabulary size\n",
    "    '''\n",
    "    # Create a python dictionary to map each character to an index 0-26.\n",
    "    string = ''.join(x)\n",
    "    chars = list(string)  # Create a list of unique characters (i.e., a to z and \\n newline)\n",
    "    chars_set = set(chars)\n",
    "    n_v = len(chars_set)  # Vocabulary size\n",
    "    char_to_ix = {ch: ii for ii, ch in enumerate(sorted(chars_set))}\n",
    "    print(char_to_ix)\n",
    "\n",
    "    return char_to_ix, n_v\n",
    "\n",
    "def listOfStr_to_array(x, Tx, func):\n",
    "    '''\n",
    "    Arguments:\n",
    "    x -- list of strings.  \n",
    "    Tx -- maximum length of string\n",
    "    func -- function to map characters to integers\n",
    "    \n",
    "    Returns:\n",
    "    x_out -- numpy array. Shape = (m, Tx).\n",
    "    '''\n",
    "\n",
    "    x_out = np.ones((len(x), Tx), dtype=np.int64)*-1  # Create empty array to store one-hot encoding result\n",
    "    for ii, string in enumerate(x):  # Iterate over each string in the file\n",
    "        x_array = np.array(list(map(func, string)))  # Map characters to integers\n",
    "        if len(x_array) <= Tx:\n",
    "            x_out[ii, : len(x_array)] = x_array\n",
    "        else:\n",
    "            x_out[ii, :] = x_array[: Tx]\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
      "x_traindev is of type <class 'numpy.ndarray'> and of shape (32513, 448)\n"
     ]
    }
   ],
   "source": [
    "char_to_ix, n_v = char2ix(x_traindev)\n",
    "x_traindev = listOfStr_to_array(x_traindev, Tx, lambda x: char_to_ix[x])\n",
    "\n",
    "# Check type and shape x_traindev\n",
    "print('x_traindev is of type {} and of shape {}'.format(type(x_traindev), x_traindev.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the y labels into an integer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_traindev = np.array(list(map(int, y_traindev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle stratify-split the data into train and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train is of type <class 'numpy.ndarray'> and of shape (26010, 448)\n",
      "x_dev is of type <class 'numpy.ndarray'> and of shape (6503, 448)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_dev, y_train, y_dev = train_test_split(x_traindev, \n",
    "                                                  y_traindev,\n",
    "                                                  test_size=0.20, \n",
    "                                                  random_state=1,\n",
    "                                                  shuffle=True, \n",
    "                                                  stratify=y_traindev)\n",
    "# Check type and shape x_train, x_dev, y_train_onehot, y_dev_onehot\n",
    "print('x_train is of type {} and of shape {}'.format(type(x_train), x_train.shape))\n",
    "print('x_dev is of type {} and of shape {}'.format(type(x_dev), x_dev.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of calsses are imbalanced, we compute and apply class weights to the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "{0: 434, 1: 2767, 2: 1177, 3: 3218, 4: 1870, 5: 1826, 6: 3381, 7: 4078, 8: 2907, 9: 784, 10: 2442, 11: 1126}\n"
     ]
    }
   ],
   "source": [
    "#Compute class weights using y_train     \n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print('Class distribution:')\n",
    "print(dict(zip(unique, counts)))\n",
    "class_weight_vec = compute_class_weight('balanced',np.array(range(12)),y_train)\n",
    "class_weight_dict = {ii: w for ii, w in enumerate(class_weight_vec)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert training and dev labels to one hot matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a list of characters into one-hot encoding numpy array\n",
    "def convert_to_one_hot(x, n):\n",
    "    '''\n",
    "    Arguments:\n",
    "    x -- array of integers. \n",
    "    n -- number of unique values. scalar value.\n",
    "   \n",
    "    Returns:\n",
    "    y -- one hot encoded matrix of integers. shape = (len(x), n).\n",
    "    '''\n",
    "\n",
    "    y = np.eye(n)[x.reshape(-1)]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_onehot is of type <class 'numpy.ndarray'> and of shape (26010, 12)\n",
      "y_dev_onehot is of type <class 'numpy.ndarray'> and of shape (6503, 12)\n"
     ]
    }
   ],
   "source": [
    "y_train_onehot = convert_to_one_hot(y_train, n_c)\n",
    "y_dev_onehot = convert_to_one_hot(y_dev, n_c)\n",
    "print('y_train_onehot is of type {} and of shape {}'.format(type(y_train_onehot), y_train_onehot.shape))\n",
    "print('y_dev_onehot is of type {} and of shape {}'.format(type(y_dev_onehot), y_dev_onehot.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the machine learning model as follows: \n",
    "\n",
    "CNN (4 layer) --> LSTM (2 layer) --> Dense (1 layer) --> Softmax --> Cross entropy loss\n",
    "\n",
    "1. CNN is suitable for character level sequence (or time series) classification. It helps to extract relevant patterns from the sequences along the feature and time dimensions.\n",
    "2. LSTM helps to recognise sequential information\n",
    "3. Fully connected Dense layer with Softmax reduces the output to 12 desired class probability distribution\n",
    "4. Cross entropy loss is used as the cost for this multiclass classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x, numOfVocab):\n",
    "    '''\n",
    "    Arguments:\n",
    "    x -- input array of shape (m,Tx) where m = len(x)\n",
    "    numOfVocab -- vocabulary size\n",
    "    \n",
    "    Returns:\n",
    "    return -- output array of shape (m, Tx, numOfVocab)\n",
    "    '''\n",
    "    import tensorflow as tf\n",
    "    return tf.to_float(tf.one_hot(x, numOfVocab, on_value=1, off_value=0, axis=-1))\n",
    "\n",
    "def one_hot_outshape(x):\n",
    "    '''\n",
    "    Arguments:\n",
    "    x -- input shape\n",
    "    \n",
    "    Returns:\n",
    "    return -- one hot encoded shape of x, i.e., (m, Tx, numOfVocab)     \n",
    "    '''\n",
    "    return x[0], x[1], 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 448)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 448, 26)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 448, 64)           8384      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 448, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 224, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 224, 64)           12352     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 224, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 112, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 112, 128)          24704     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 112, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 56, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 56, 128)           49280     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 56, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 28, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 28, 256)           263168    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                1548      \n",
      "=================================================================\n",
      "Total params: 556,556\n",
      "Trainable params: 556,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building the model')\n",
    "kernel_length = [ 5,   3,   3,   3]\n",
    "filter_num    = [64,  64, 128, 128]\n",
    "pool_length   = [ 2,   2,   2,   2]\n",
    "x_input = Input(shape=(Tx,), dtype='int64')\n",
    "# The one-hot encoded vectors for each input string is built on the fly using the Lambda layer\n",
    "# Desired shape of input data X = (m, Tx, n_v)\n",
    "x_one_hot = Lambda(one_hot, output_shape=one_hot_outshape, arguments={'numOfVocab': 26})(x_input)\n",
    "x_conv = x_one_hot\n",
    "for ii in range(len(filter_num)):\n",
    "    x_conv = Conv1D(filters=filter_num[ii],\n",
    "                    kernel_size=kernel_length[ii],\n",
    "                    padding='same')(x_conv)\n",
    "    x_conv = Activation('relu')(x_conv)\n",
    "    x_conv = MaxPooling1D(pool_size=pool_length[ii])(x_conv)\n",
    "\n",
    "x_lstm1 = Bidirectional(LSTM(128, return_sequences=True))(x_conv)\n",
    "x_lstm1_dropout = Dropout(0.3)(x_lstm1)\n",
    "x_lstm2 = LSTM(128, return_sequences=False)(x_lstm1_dropout)\n",
    "x_lstm2_dropout = Dropout(0.3)(x_lstm2)\n",
    "x_out = Dense(n_c, activation='softmax')(x_lstm2_dropout)\n",
    "model = Model(inputs=x_input, outputs=x_out, name='textClassifier')\n",
    "# Check model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model at regular epoch intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint('./cnn_lstm'+'-{epoch:02d}-{val_acc:.2f}.hdf5',\n",
    "                                             monitor='val_loss', \n",
    "                                             verbose=1, \n",
    "                                             save_best_only=False, \n",
    "                                             save_weights_only=False, \n",
    "                                             mode='auto', \n",
    "                                             period=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Fitting the model now')\n",
    "model.fit(x_train, y_train_onehot,\n",
    "          validation_data=(x_dev, y_dev_onehot),\n",
    "          batch_size=256, \n",
    "          epochs=130,\n",
    "          shuffle=True,\n",
    "          callbacks=[checkpoint],\n",
    "          class_weight=class_weight_dict)\n",
    "\n",
    "# Save model to continue later\n",
    "model.save('./cnn_lstm.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have already trained the above model over 180 epochs, which yielded a training set accuracy of ~99% and validation set accuracy of ~87%.\n",
    "\n",
    "Hence, we now load the trained model containing the trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model to continue testing\n",
    "#Our saved file is named \"cnn_lstm-180-0.87.hdf5\"\n",
    "model = load_model('./cnn_lstm-180-0.87.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check accuracy on training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, acc_train = model.evaluate(x_train, y_train_onehot)\n",
    "_, acc_dev = model.evaluate(x_dev, y_dev_onehot)\n",
    "print('Train set accuracy: {}'.format(acc_train))\n",
    "print('Validation set accuracy: {}'.format(acc_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test is of type <class 'numpy.ndarray'> and of shape (3000, 448)\n"
     ]
    }
   ],
   "source": [
    "#Read in the test set\n",
    "x_test = readIn('./xtest.txt')\n",
    "x_test = listOfStr_to_array(x_test, Tx, lambda x: char_to_ix[x])\n",
    "\n",
    "# Check type and shape x_test\n",
    "print('x_test is of type {} and of shape {}'.format(type(x_test), x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction on test set and write the predcited class labels of test sample into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write2file(filename,resultList):\n",
    "    with open(filename,'w',newline='') as outputFile:\n",
    "        writer = csv.writer(outputFile)\n",
    "        for result in resultList:\n",
    "            writer.writerow([result]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "#Write predictions to file\n",
    "pred_labels = np.argmax(pred,axis=1)\n",
    "write2file('ytest.txt',pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
